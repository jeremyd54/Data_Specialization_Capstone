---
title: "Capstone Initial Milestone Report"
author: "Jeremy Dean"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(lemon)
library(tm)
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(gtable)
library(gridExtra)
knit_print.data.frame <- lemon_print
```

## Introduction

This paper is the initial report for the Coursera Data Science Specialization capstone project offered through John Hopkins University. The purpose of the project is to create a prediction algorithm and ultimately an app that takes a word, or words, as inputs and predicts a word to come after. There are three data sets that will be used to train this algorithm. The sources for these data sets are Twitter, blogs, and news sites. This paper will be covering the downloading, pre-processing, and conducting exploratory analysis on the data. All code that was used can be found in the Appendix.

## Downloading and Pre-processing

The data sets can be found at: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip. After downloading, the zip file was decompressed and made available. Pre-processing was simply converting the word-sets into a "tall" format. That is, they were saved (or tokenized) in a data frame in which each row contained only one or two words. The R package "tidytext" was used to do this and utilized through the rest of this analysis.

```{r pre}

twitLines <- readLines("./final/en_US/en_US.twitter.txt")
twit <- tibble(entry = 1:length(twitLines), text = twitLines) %>%
    unnest_tokens(word, text)

blogLines <- readLines("./final/en_US/en_US.blogs.txt")
blog <- tibble(entry = 1:length(blogLines), text = blogLines) %>%
    unnest_tokens(word, text)

newsLines <- readLines("./final/en_US/en_US.news.txt")
news <- tibble(entry = 1:length(newsLines), text = newsLines) %>%
    unnest_tokens(word, text)

```

## Common Words

The first point of interest in my exploratory analysis is finding the most common words in each of the data sets. This is an easy process in tidytext with the count() function. 

```{r caption="Top 10 Twitter Words", render=lemon_print}
twitCommon <- twit %>%
    count(word, sort = TRUE)
head(twitCommon, n = 10)
```

Unsurprisingly, the most common words are "and", "the, "to", etc. These are called stop words and will not be very useful for predictions. Here's twitter again with stop words removed:

```{r caption="Twitter Without Stop Words", render=lemon_print}
twitCommonStop <- twit %>%
    anti_join(stop_words) %>%
    count(word, sort = TRUE)
head(twitCommonStop, n = 10)
```

"Love" is the most common word on Twitter. Also on the list are "rt" (short for re-tweet) and "lol" and these may not be useful for predictions and perhaps should be removed down the road. The top-20 words for each source are shown below.

```{r commonPlots}
blogCommon <- blog %>%
    count(word, sort = TRUE)
blogCommonStop <- blog %>%
    anti_join(stop_words) %>%
    count(word, sort = TRUE)
newsCommon <- news %>%
    count(word, sort = TRUE)
newsCommonStop <- news %>%
    anti_join(stop_words) %>%
    count(word, sort = TRUE)
twitComP1 <- twitCommon %>% 
    filter(n > 168000) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("With Stop Words") +
    geom_col(fill = "blue")   
twitComP2 <- twitCommonStop %>%
    filter(n > 28450) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("Stop Words Removed") +
    geom_col(fill = "red")
blogComP1 <- blogCommon %>%
    filter(n > 218000) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("With Stop Words") + 
    geom_col(fill = "blue")
blogComP2 <- blogCommonStop %>%
    filter(n > 21400) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("Stop Words Removed") + 
    geom_col(fill = "red")
newsComP1 <- newsCommon %>%
    filter(n > 11600) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("With Stop Words") +
    geom_col(fill = "blue")
newsComP2 <- newsCommonStop %>%
    filter(n > 1797) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("Stop Words Removed") +
    geom_col(fill = "red")
rm(blogCommonStop, newsCommonStop, twitCommonStop,
   blogCommon, newsCommon, twitCommon, blog, news, twit)
grid.arrange(twitComP1,twitComP2, nrow = 1, 
             top = "Most Common Twitter Words")
grid.arrange(blogComP1, blogComP2, nrow = 1, 
             top = "Most Common Blog Words")
grid.arrange(newsComP1, newsComP2, nrow = 1,
             top = "Most Common News Words")
```

With stop words, the sources have more or less the same top-20 list. Other common occurrences among all three sources are numbers and the letter "a" with a hat accent, suggesting a non-English character. Removing these should definitely be considered.

## Bi-Grams

Another part of the data I looked at was so-called bi-grams. Bi-grams are simply two words grouped together. These will most likely be extremely useful in generating the prediction algorithm. When looking at these bi-grams, I removed stop words and non-English alphabet characters. Below are the top-20 bi-grams in the three sources.

```{r twitBi}
load("twitBiCom.RData")
twitBiPlot <- twitBiCom %>%
    filter(n > 1314) %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(n, bigram)) + ggtitle("Twitter") +
    geom_col(fill = "red")
rm(twitBiCom)
```

```{r blogBi}
load("blogBiCom.RData")
blogBiPlot <- blogBiCom %>%
    filter(n > 840) %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(n, bigram)) + ggtitle("Blogs") +
    geom_col(fill = "red")
rm(blogBiCom)
```

```{r newsBi}
load("newsBiCom.RDAta")
newsBiPlot <- newsBiCom %>%
    filter(n > 126) %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(n, bigram)) + ggtitle("News") +
    geom_col(fill = "red")
rm(newsBiCom)
```

```{r biPlots}
grid.arrange(twitBiPlot, blogBiPlot, newsBiPlot, nrow = 1,
             top = "Common Bi-Grams")
```

These graphs show the most common two-word combinations and most of them will be fairly good predictors. In the blogs set, a number followed by "minutes" or two numbers are half of the top-20 further demonstrating that removing numbers could be a useful tool going forward.

## Building A Model

I plan to build a model on these word sources by first removing the non-English characters and perhaps numbers as well. Then, starting with bi-grams, I will use the first word as input and the second as a response. Using machine learning, I will fit models to see which has the best accuracy on test sets. Then I will expand on the model with tri-grams and so on.


## Appendex

A special thanks to the people at https://www.tidytextmining.com/tidytext.html. The inspiration for much of my code was from them.

### A Note On Memory

Since the data set were quite large, some operations took a very long time. With 16 Gb of RAM, retaining all the data objects caused the memory to overfill. Thus, the code contains saving data frames to the disc and removing them for memory. This also allows them to be reloaded quickly in the future.

### Code Book

In order to print pretty tables in this report the setup is as such:

```{r,eval=FALSE, echo=TRUE}
library(lemon)
knit_print.data.frame <- lemon_print

{r, render=lemon_print}
```

#### intial_data_processing.R
```{r intial, eval=FALSE,echo=TRUE}
library(tm)
library(dplyr)
library(tidyr)
library(tidytext)

## Download and unzip data
dataUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(dataUrl, destfile = "data.zip")
unzip("data.zip")

#Read in data and transform into tall format

twitLines <- readLines("./final/en_US/en_US.twitter.txt")
twit <- tibble(entry = 1:length(twitLines), text = twitLines) %>%
    unnest_tokens(word, text)

blogLines <- readLines("./final/en_US/en_US.blogs.txt")
blog <- tibble(entry = 1:length(blogLines), text = blogLines) %>%
    unnest_tokens(word, text)

newsLines <- readLines("./final/en_US/en_US.news.txt")
news <- tibble(entry = 1:length(newsLines), text = newsLines) %>%
    unnest_tokens(word, text)

```

#### exploratory_analysis.R
```{r,eval=FALSE,echo=TRUE}
source("intial_data_processing.R")
library(ggplot2)
library(gtable)
library(gridExtra)

## Import stop words
data("stop_words")

### Find common words

## twitter
twitCommon <- twit %>%
    count(word, sort = TRUE)
head(twitCommon, n = 20)
#Remove stop words
twitCommonStop <- twit %>%
    anti_join(stop_words) %>%
    count(word, sort = TRUE)
head(twitCommonStop, n = 20)

## Blogs
blogCommon <- blog %>%
    count(word, sort = TRUE)
head(blogCommon, n = 20)
#Remove stop words
blogCommonStop <- blog %>%
    anti_join(stop_words) %>%
    count(word, sort = TRUE)
head(blogCommonStop, n = 20)

## News
newsCommon <- news %>%
    count(word, sort = TRUE)
head(newsCommon, n = 20)
#Remove stop words
newsCommonStop <- news %>%
    anti_join(stop_words) %>%
    count(word, sort = TRUE)
head(newsCommonStop, n = 20)

## Plot common words
#Twitter
twitComP1 <- twitCommon %>% 
    filter(n > 168000) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("With Stop Words") +
    geom_col(fill = "blue")   
twitComP2 <- twitCommonStop %>%
    filter(n > 28450) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("Stop Words Removed") +
    geom_col(fill = "red")
grid.arrange(twitComP1,twitComP2, nrow = 1, 
             top = "Most Common Twitter Words")
#Blogs
blogComP1 <- blogCommon %>%
    filter(n > 218000) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("With Stop Words") + 
    geom_col(fill = "blue")
blogComP2 <- blogCommonStop %>%
    filter(n > 21400) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("Stop Words Removed") + 
    geom_col(fill = "red")
grid.arrange(blogComP1, blogComP2, nrow = 1, 
             top = "Most Common Blog Words")
#News
newsComP1 <- newsCommon %>%
    filter(n > 11600) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("With Stop Words") +
    geom_col(fill = "blue")
newsComP2 <- newsCommonStop %>%
    filter(n > 1797) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) + ggtitle("Stop Words Removed") +
    geom_col(fill = "red")
grid.arrange(newsComP1, newsComP2, nrow = 1,
             top = "Most Common News Words")

## Clear out RAM space
save(blog, news, twit, file = "sources.RData")
save(blogCommon, newsCommon, twitCommon, file = "common.RData")
save(blogCommonStop, newsCommonStop, twitCommonStop, file = "commonNoStop.RData")
rm(blogCommonStop, newsCommonStop, twitCommonStop,
   blogCommon, newsCommon, twitCommon, blog, news, twit)

### Bi-Grams

## Twitter
#Make data frame
twitBi <- tibble(entry = 1:length(twitLines), text = twitLines) %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word)
#Save to disc
save(twitBi, file = "twitBi.RData")
#Count commons
twitBiCom <- twitBi %>% 
    count(word1, word2, sort = TRUE) %>%
    unite(bigram, word1, word2, sep = " ")
#Clear space and look
rm(twitBi)
head(twitBiCom, n = 10)
#Remove non-characters and view
twitBiCom <- twitBiCom[grep("^[0-9a-z ]+$", twitBiCom$bigram), ]
head(twitBiCom, n = 20)
#Plot
twitBiPlot <- twitBiCom %>%
    filter(n > 1314) %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(n, bigram)) + ggtitle("Twitter") +
    geom_col(fill = "red")
#Save and remove
save(twitBiCom, file = "twitBiCom.RData")
rm(twitBiCom)

### Blog
#Make data frame and save
blogBi <- tibble(text = blogLines) %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word)
save(blogBi, file = "blogBi.RData")
##Count Commons and clear space
blogBiCom <- blogBi %>%
    count(word1, word2, sort = TRUE) %>%
    unite(bigram, word1, word2, sep = " ")
#Clear space and view
rm(blogBi)
head(blogBiCom, n = 10)
##Remove non-characters and view
blogBiCom <- blogBiCom[grep("^[0-9a-z ]+$", blogBiCom$bigram), ]
head(blogBiCom, n = 20)
#Plot
blogBiPlot <- blogBiCom %>%
    filter(n > 840) %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(n, bigram)) + ggtitle("Blogs") +
    geom_col(fill = "red")
save(blogBiCom, file = "blogBiCom.RData")
rm(blogBiCom)

## News

#Make data frame and save
newsBi <- tibble(text = newsLines) %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word)
save(newsBi, file = "newsBi.RData")
#Count common and clear
newsBiCom <- newsBi %>%
    count(word1, word2, sort = TRUE) %>%
    unite(bigram, word1, word2, sep = " ")
#View and clear space
head(newsBiCom, n = 10)
rm(newsBi)
#Remove non-characters
newsBiCom <- newsBiCom[grep("^[0-9a-z ]+$", newsBiCom$bigram), ]
head(newsBiCom, n = 20)
save(newsBiCom, file = "newsBiCom.RDAta")
#Plot
newsBiPlot <- newsBiCom %>%
    filter(n > 126) %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(n, bigram)) + ggtitle("News") +
    geom_col(fill = "red")
rm(newsBiCom)

## Plot together
grid.arrange(twitBiPlot, blogBiPlot, newsBiPlot, nrow = 1,
             top = "Common Bi-Grams")

